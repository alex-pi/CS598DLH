{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4f64a7c4aa8c138f2a216c055f299fb9",
     "grade": false,
     "grade_id": "cell-757f7449dc485bc3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# HW1\n",
    "\n",
    "## Overview\n",
    "\n",
    "Preparing the data, computing basic statistics and constructing simple models are essential steps for data science practice. In this homework, you will use clinical data as raw input to perform **Heart Failure Prediction**. For this homework, **Python** programming will be required. See the attached skeleton code as a start-point for the programming questions.\n",
    "\n",
    "This homework assumes familiarity with Pandas. If you need a Pandas crash course, we recommend working through [100 Pandas Puzzles](https://github.com/ajcr/100-pandas-puzzles), the solutions are also available at that link. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a33f4260efff941e320b24c95157b0e1",
     "grade": false,
     "grade_id": "cell-f4a83de52abb8394",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-17T04:29:59.299540Z",
     "start_time": "2022-01-17T04:29:59.291328Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "17cd37dbc82c1a46157ec9adb81a3844",
     "grade": false,
     "grade_id": "cell-10db081f94b98d02",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "DATA_PATH = \"./data/\"\n",
    "TRAIN_DATA_PATH = DATA_PATH + \"train/\"\n",
    "VAL_DATA_PATH = DATA_PATH + \"val/\"\n",
    "    \n",
    "sys.path.append(\"./\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a053ad27ddd1d1d271a6d28c51c634bc",
     "grade": false,
     "grade_id": "cell-52ddaebbf4a162a4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "85cba0bf235d0a3f169d8bd9cf87350c",
     "grade": false,
     "grade_id": "cell-e59364a32d9b5fc9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## About Raw Data\n",
    "\n",
    "For this homework, we will be using a clinical dataset synthesized from [MIMIC-III](https://www.nature.com/articles/sdata201635).\n",
    "\n",
    "Navigate to `TRAIN_DATA_PATH`. There are three CSV files which will be the input data in this homework. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-17T04:29:59.429435Z",
     "start_time": "2022-01-17T04:29:59.302220Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7fdd12a66d23c25566d9167b74f1f34e",
     "grade": false,
     "grade_id": "cell-d92e9dc996c61070",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'ls' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!ls $TRAIN_DATA_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "46301c39a6360e38c7814cb9f4f1519a",
     "grade": false,
     "grade_id": "cell-d9212ce110d4dc0b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**events.csv**\n",
    "\n",
    "The data provided in *events.csv* are event sequences. Each line of this file consists of a tuple with the format *(pid, event_id, vid, value)*. \n",
    "\n",
    "For example, \n",
    "\n",
    "```\n",
    "33,DIAG_244,0,1\n",
    "33,DIAG_414,0,1\n",
    "33,DIAG_427,0,1\n",
    "33,LAB_50971,0,1\n",
    "33,LAB_50931,0,1\n",
    "33,LAB_50812,1,1\n",
    "33,DIAG_425,1,1\n",
    "33,DIAG_427,1,1\n",
    "33,DRUG_0,1,1\n",
    "33,DRUG_3,1,1\n",
    "```\n",
    "\n",
    "- **pid**: De-identified patient identier. For example, the patient in the example above has pid 33. \n",
    "- **event_id**: Clinical event identifier. For example, DIAG_244 means the patient was diagnosed of disease with ICD9 code [244](http://www.icd9data.com/2013/Volume1/240-279/240-246/244/244.htm); LAB_50971 means that the laboratory test with code 50971 was conducted on the patient; and DRUG_0 means that a drug with code 0 was prescribed to the patient. Corresponding lab (drug) names can be found in `{DATA_PATH}/lab_list.txt` (`{DATA_PATH}/drug_list.txt`).\n",
    "- **vid**: Visit identifier. For example, the patient has two visits in total. Note that vid is ordinal. That is, visits with bigger vid occour after that with smaller vid.\n",
    "- **value**: Contains the value associated to an event (always 1 in the synthesized dataset)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "dd71df788c3b463c5db5d09b8991575d",
     "grade": false,
     "grade_id": "cell-b20f2e82052a2926",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**hf_events.csv**\n",
    "\n",
    "The data provided in *hf_events.csv* contains pid of patients who have been diagnosed with heart failure (i.e., DIAG_398, DIAG_402, DIAG_404, DIAG_428) in at least one visit. They are in the form of a tuple with the format *(pid, vid, label)*. For example,\n",
    "\n",
    "```\n",
    "156,0,1\n",
    "181,1,1\n",
    "```\n",
    "\n",
    "The vid indicates the index of the first visit with heart failure of that patient and a label of 1 indicates the presence of heart failure. **Note that only patients with heart failure are included in this file. Patients who are not mentioned in this file have never been diagnosed with heart failure.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c5d154237645fcb55795c002c2c56c9f",
     "grade": false,
     "grade_id": "cell-8a8108bebe9e0d39",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**event_feature_map.csv**\n",
    "\n",
    "The *event_feature_map.csv* is a map from an event_id to an integer index. This file contains *(idx, event_id)* pairs for all event ids."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4cac1012cd2d5bacf94ba5393cf3ac3b",
     "grade": false,
     "grade_id": "cell-e092836975c6797a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 1 Descriptive Statistics [20 points]\n",
    "\n",
    "Before starting analytic modeling, it is a good practice to get descriptive statistics of the input raw data. In this question, you need to write code that computes various metrics on the data described previously. A skeleton code is provided to you as a starting point.\n",
    "\n",
    "The definition of terms used in the result table are described below:\n",
    "\n",
    "- **Event count**: Number of events recorded for a given patient.\n",
    "- **Encounter count**: Number of visits recorded for a given patient.\n",
    "\n",
    "Note that every line in the input file is an event, while each visit consists of multiple events."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2c424d35d82a9f9d3dd0fc2dfca71683",
     "grade": false,
     "grade_id": "cell-a53b106557b4e31f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Complete the following code cell to implement the required statistics.**\n",
    "\n",
    "Please be aware that **you are NOT allowed to change the filename and any existing function declarations.** Only `numpy`, `scipy`, `scikit-learn`, `pandas` and other built-in modules of python will be available for you to use. The use of `pandas` library is suggested. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-17T04:29:59.718385Z",
     "start_time": "2022-01-17T04:29:59.432216Z"
    },
    "deletable": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "\n",
    "# PLEASE USE THE GIVEN FUNCTION NAME, DO NOT CHANGE IT.\n",
    "\n",
    "def read_csv(filepath=TRAIN_DATA_PATH):\n",
    "\n",
    "    '''\n",
    "    Read the events.csv and hf_events.csv files. \n",
    "    Variables returned from this function are passed as input to the metric functions.\n",
    "    \n",
    "    NOTE: remember to use `filepath` whose default value is `TRAIN_DATA_PATH`.\n",
    "    '''\n",
    "    \n",
    "    events = pd.read_csv(filepath + 'events.csv')\n",
    "    hf = pd.read_csv(filepath + 'hf_events.csv')\n",
    "\n",
    "    return events, hf\n",
    "\n",
    "def event_count_metrics(events, hf):\n",
    "\n",
    "    '''\n",
    "    TODO : Implement this function to return the event count metrics.\n",
    "    \n",
    "    Event count is defined as the number of events recorded for a given patient.\n",
    "    '''\n",
    "\n",
    "    # your code here\n",
    "    hf_pids = hf.loc[:, 'pid']\n",
    "    num_ev_by_patient = (events.groupby(['pid']).sum())\n",
    "    ev_by_norm_patient = num_ev_by_patient.loc[~num_ev_by_patient.index.isin(hf_pids), 'value']\n",
    "    ev_by_hf_patient = num_ev_by_patient.loc[num_ev_by_patient.index.isin(hf_pids), 'value']\n",
    "\n",
    "    avg_hf_event_count = ev_by_hf_patient.mean()\n",
    "    max_hf_event_count = ev_by_hf_patient.max()\n",
    "    min_hf_event_count = ev_by_hf_patient.min()\n",
    "    avg_norm_event_count = ev_by_norm_patient.mean()\n",
    "    max_norm_event_count = ev_by_norm_patient.max()\n",
    "    min_norm_event_count = ev_by_norm_patient.min()\n",
    "\n",
    "    return avg_hf_event_count, max_hf_event_count, min_hf_event_count, \\\n",
    "           avg_norm_event_count, max_norm_event_count, min_norm_event_count\n",
    "\n",
    "def encounter_count_metrics(events, hf):\n",
    "\n",
    "    '''\n",
    "    TODO : Implement this function to return the encounter count metrics.\n",
    "    \n",
    "    Encounter count is defined as the number of visits recorded for a given patient. \n",
    "    '''\n",
    "    \n",
    "    avg_hf_encounter_count = None\n",
    "    max_hf_encounter_count = None\n",
    "    min_hf_encounter_count = None\n",
    "    avg_norm_encounter_count = None\n",
    "    max_norm_encounter_count = None\n",
    "    min_norm_encounter_count = None\n",
    "    \n",
    "    # your code here\n",
    "    hf_pids = hf.loc[:, 'pid']\n",
    "    num_vis_by_patient = (events.groupby(['pid'])[['vid']].nunique())\n",
    "    num_vis_by_norm_patient = num_vis_by_patient.loc[~num_vis_by_patient.index.isin(hf_pids), 'vid']\n",
    "    num_vis_by_hf_patient = num_vis_by_patient.loc[num_vis_by_patient.index.isin(hf_pids), 'vid']\n",
    "\n",
    "    avg_hf_encounter_count = num_vis_by_hf_patient.mean()\n",
    "    max_hf_encounter_count = num_vis_by_hf_patient.max()\n",
    "    min_hf_encounter_count = num_vis_by_hf_patient.min()\n",
    "    avg_norm_encounter_count = num_vis_by_norm_patient.mean()\n",
    "    max_norm_encounter_count = num_vis_by_norm_patient.max()\n",
    "    min_norm_encounter_count = num_vis_by_norm_patient.min()\n",
    "\n",
    "    return avg_hf_encounter_count, max_hf_encounter_count, min_hf_encounter_count, \\\n",
    "           avg_norm_encounter_count, max_norm_encounter_count, min_norm_encounter_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0          33\n",
      "1          64\n",
      "2         156\n",
      "3         181\n",
      "4         199\n",
      "        ...  \n",
      "2955    99784\n",
      "2956    99788\n",
      "2957    99825\n",
      "2958    99841\n",
      "2959    99971\n",
      "Name: pid, Length: 2960, dtype: int64\n",
      "ev_by_patient=\n",
      "        vid  value\n",
      "pid              \n",
      "33      75    122\n",
      "64      57    132\n",
      "78      81    209\n",
      "156     70    187\n",
      "181     64    165\n",
      "...    ...    ...\n",
      "99788   54    107\n",
      "99800   79    161\n",
      "99825  450    238\n",
      "99841  215    192\n",
      "99971   71    143\n",
      "\n",
      "[4000 rows x 2 columns]\n",
      "num_vis_by_patient=\n",
      "        vid\n",
      "pid       \n",
      "33       2\n",
      "64       2\n",
      "78       2\n",
      "156      2\n",
      "181      2\n",
      "...    ...\n",
      "99788    2\n",
      "99800    2\n",
      "99825    4\n",
      "99841    3\n",
      "99971    2\n",
      "\n",
      "[4000 rows x 1 columns]\n",
      "pid\n",
      "78       2\n",
      "190      2\n",
      "333      2\n",
      "695      2\n",
      "701      2\n",
      "        ..\n",
      "99102    2\n",
      "99381    2\n",
      "99586    2\n",
      "99729    2\n",
      "99800    2\n",
      "Name: vid, Length: 1040, dtype: int64\n",
      "2.189423076923077\n",
      "11\n",
      "1\n",
      "2.8060810810810812\n",
      "34\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "events, hf = read_csv(TRAIN_DATA_PATH)\n",
    "#print(events.shape[0])\n",
    "#print(events.head())\n",
    "\n",
    "hf_pids = hf.loc[:, 'pid']\n",
    "print(hf_pids)\n",
    "ev_by_patient = (events.groupby(['pid']).sum())\n",
    "print(\"ev_by_patient=\\n\", ev_by_patient)\n",
    "\n",
    "ev_by_norm_patient = ev_by_patient.loc[~ev_by_patient.index.isin(hf_pids), 'value']\n",
    "#print(\"ev_by_norm_patient=\\n\", ev_by_norm_patient)\n",
    "\n",
    "#print(ev_by_norm_patient.mean())\n",
    "#print(ev_by_norm_patient.max())\n",
    "#print(ev_by_norm_patient.min())\n",
    "\n",
    "ev_by_hf_patient = ev_by_patient.loc[ev_by_patient.index.isin(hf_pids), 'value']\n",
    "\n",
    "#print(ev_by_hf_patient.mean())\n",
    "#print(ev_by_hf_patient.max())\n",
    "#print(ev_by_hf_patient.min())\n",
    "\n",
    "num_vis_by_patient = (events.groupby(['pid'])[['vid']].nunique())\n",
    "print(\"num_vis_by_patient=\\n\", num_vis_by_patient)\n",
    "\n",
    "num_vis_by_norm_patient = num_vis_by_patient.loc[~num_vis_by_patient.index.isin(hf_pids), 'vid']\n",
    "print(num_vis_by_norm_patient)\n",
    "\n",
    "print(num_vis_by_norm_patient.mean())\n",
    "print(num_vis_by_norm_patient.max())\n",
    "print(num_vis_by_norm_patient.min())\n",
    "\n",
    "num_vis_by_hf_patient = num_vis_by_patient.loc[num_vis_by_patient.index.isin(hf_pids), 'vid']\n",
    "#print(num_vis_by_hf_patient)\n",
    "\n",
    "print(num_vis_by_hf_patient.mean())\n",
    "print(num_vis_by_hf_patient.max())\n",
    "print(num_vis_by_hf_patient.min())\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     vid\n",
      "pid     \n",
      "1      2\n",
      "2      3\n",
      "3      1\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame({'pid': [1, 1, 2, 2, 2, 3], 'vid': [0, 1, 0, 1, 5, 0]})\n",
    "x = df.groupby(['pid']).nunique()\n",
    "print(x)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-17T04:30:00.227147Z",
     "start_time": "2022-01-17T04:29:59.720539Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9e7427796cf3b4d5a493bd096d082ac9",
     "grade": false,
     "grade_id": "cell-d28045e592b8f081",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   pid  event_id  vid  value\n",
      "0   33  DIAG_244    0      1\n",
      "1   33  DIAG_414    0      1\n",
      "2   33  DIAG_427    0      1\n",
      "3   33  DIAG_585    0      1\n",
      "4   33  DIAG_V45    0      1\n",
      "Time to compute event count metrics: 0.2897145748138428s\n",
      "(188.9375, 2046, 28, 118.64423076923077, 1014, 6)\n",
      "Time to compute encounter count metrics: 0.4781181812286377s\n",
      "(2.8060810810810812, 34, 2, 2.189423076923077, 11, 1)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "DO NOT MODIFY THIS.\n",
    "'''\n",
    "\n",
    "events, hf = read_csv(TRAIN_DATA_PATH)\n",
    "print(events.head())\n",
    "\n",
    "#Compute the event count metrics\n",
    "start_time = time.time()\n",
    "event_count = event_count_metrics(events, hf)\n",
    "end_time = time.time()\n",
    "print((\"Time to compute event count metrics: \" + str(end_time - start_time) + \"s\"))\n",
    "print(event_count)\n",
    "\n",
    "#Compute the encounter count metrics\n",
    "start_time = time.time()\n",
    "encounter_count = encounter_count_metrics(events, hf)\n",
    "end_time = time.time()\n",
    "print((\"Time to compute encounter count metrics: \" + str(end_time - start_time) + \"s\"))\n",
    "print(encounter_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-17T04:30:00.559437Z",
     "start_time": "2022-01-17T04:30:00.229054Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6d140e954c3a28d1e5479935f3ef713e",
     "grade": true,
     "grade_id": "cell-8f9c85ba731d96e5",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "AUTOGRADER CELL. DO NOT MODIFY THIS.\n",
    "'''\n",
    "\n",
    "events, hf = read_csv(TRAIN_DATA_PATH)\n",
    "event_count = event_count_metrics(events, hf)\n",
    "assert event_count == (188.9375, 2046, 28, 118.64423076923077, 1014, 6), \"event_count failed!\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-17T04:30:01.151817Z",
     "start_time": "2022-01-17T04:30:00.561593Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "557341b6198f8525680acb1753cbbdbf",
     "grade": true,
     "grade_id": "cell-343b745247b90367",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "AUTOGRADER CELL. DO NOT MODIFY THIS.\n",
    "'''\n",
    "\n",
    "events, hf = read_csv(TRAIN_DATA_PATH)\n",
    "encounter_count = encounter_count_metrics(events, hf)\n",
    "assert encounter_count == (2.8060810810810812, 34, 2, 2.189423076923077, 11, 1), \"encounter_count failed!\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d67dbea2960c373fa33683228f344817",
     "grade": false,
     "grade_id": "cell-c5413d29c37a0a33",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 2 Feature construction [40 points] \n",
    "\n",
    "It is a common practice to convert raw data into a standard data format before running real machine learning models. In this question, you will implement the necessary python functions in this script. You will work with *events.csv*, *hf_events.csv* and *event_feature_map.csv* files provided in **TRAIN_DATA_PATH** folder. The use of `pandas` library in this question is recommended. \n",
    "\n",
    "Listed below are a few concepts you need to know before beginning feature construction (for details please refer to lectures). \n",
    "\n",
    "<img src=\"img/window.jpg\" width=\"600\"/>\n",
    "\n",
    "- **Index vid**: Index vid is evaluated as follows:\n",
    "  - For heart failure patients: Index vid is the vid of the first visit with heart failure for that patient (i.e., vid field in *hf_events.csv*). \n",
    "  - For normal patients: Index vid is the vid of the last visit for that patient (i.e., vid field in *events.csv*). \n",
    "- **Observation Window**: The time interval you will use to identify relevant events. Only events present in this window should be included while constructing feature vectors.\n",
    "- **Prediction Window**: A fixed time interval that is to be used to make the prediction.\n",
    "\n",
    "In the example above, the index vid is 3. Visits with vid 0, 1, 2 are within the observation window. The prediction window is between visit 2 and 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e0a8860140e64060ac50e4ca3c701ea1",
     "grade": false,
     "grade_id": "cell-aa768bbdeed84907",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 2.1 Compute the index vid [10 points]\n",
    "\n",
    "Use the definition provided above to compute the index vid for all patients. Complete the method `read_csv` and `calculate_index_vid` provided in the following code cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-17T04:30:01.160227Z",
     "start_time": "2022-01-17T04:30:01.154297Z"
    },
    "deletable": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "\n",
    "def read_csv(filepath=TRAIN_DATA_PATH):\n",
    "    \n",
    "    '''\n",
    "    Read the events.csv, hf_events.csv and event_feature_map.csv files.\n",
    "    \n",
    "    NOTE: remember to use `filepath` whose default value is `TRAIN_DATA_PATH`.\n",
    "    '''\n",
    "\n",
    "    events = pd.read_csv(filepath + 'events.csv')\n",
    "    hf = pd.read_csv(filepath + 'hf_events.csv')\n",
    "    feature_map = pd.read_csv(filepath + 'event_feature_map.csv')\n",
    "\n",
    "    return events, hf, feature_map\n",
    "\n",
    "\n",
    "def calculate_index_vid(events, hf):\n",
    "    \n",
    "    '''\n",
    "    TODO: This function needs to be completed.\n",
    "\n",
    "    Suggested steps:\n",
    "        1. Create list of normal patients (hf_events.csv only contains information about heart failure patients).\n",
    "        2. Split events into two groups based on whether the patient has heart failure or not.\n",
    "        3. Calculate index vid for each patient.\n",
    "    \n",
    "    IMPORTANT:\n",
    "        `indx_vid` should be a pd dataframe with header `['pid', 'indx_vid']`.\n",
    "    '''\n",
    "\n",
    "    indx_vid = ''\n",
    "    \n",
    "    # your code here\n",
    "    hf_pids = hf.loc[:, ['pid']]\n",
    "    ev_by_norm_patient = events.loc[~events['pid'].isin(hf_pids['pid']), :]\n",
    "    ivid_norm_patient = ev_by_norm_patient.groupby(['pid'])[['vid']].max()\n",
    "    ivid_norm_patient.reset_index(drop=False, inplace=True)\n",
    "\n",
    "    ev_by_hf_patient = pd.merge(events, hf,  how='inner',\n",
    "                          left_on=['pid','vid'], right_on = ['pid','vid'])\n",
    "\n",
    "    ivid_hf_patient = ev_by_hf_patient.drop_duplicates(subset=['pid', 'vid'])\n",
    "    ivid_hf_patient = ivid_hf_patient.loc[: , ['pid', 'vid']]\n",
    "    ivid_hf_patient.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    indx_vid = pd.concat([ivid_norm_patient, ivid_hf_patient])\n",
    "    indx_vid.reset_index(drop=True, inplace=True)\n",
    "    indx_vid.rename({'vid': 'indx_vid'}, axis=1, inplace=True)\n",
    "    \n",
    "    return indx_vid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2960, 3)\n",
      "        pid  vid\n",
      "0        78    1\n",
      "1       190    1\n",
      "2       333    1\n",
      "3       695    1\n",
      "4       701    1\n",
      "...     ...  ...\n",
      "1035  99102    1\n",
      "1036  99381    1\n",
      "1037  99586    1\n",
      "1038  99729    1\n",
      "1039  99800    1\n",
      "\n",
      "[1040 rows x 2 columns]\n",
      "(201290, 5)\n",
      "        pid  vid\n",
      "0        33    0\n",
      "1        64    0\n",
      "2       156    0\n",
      "3       181    1\n",
      "4       199    1\n",
      "...     ...  ...\n",
      "2955  99784    2\n",
      "2956  99788    1\n",
      "2957  99825    1\n",
      "2958  99841    0\n",
      "2959  99971    1\n",
      "\n",
      "[2960 rows x 2 columns]\n",
      "        pid  indx_vid\n",
      "0        78         1\n",
      "1       190         1\n",
      "2       333         1\n",
      "3       695         1\n",
      "4       701         1\n",
      "...     ...       ...\n",
      "3995  99784         2\n",
      "3996  99788         1\n",
      "3997  99825         1\n",
      "3998  99841         0\n",
      "3999  99971         1\n",
      "\n",
      "[4000 rows x 2 columns]\n",
      "      pid  indx_vid\n",
      "1040   33         0\n"
     ]
    }
   ],
   "source": [
    "print(hf.shape)\n",
    "\n",
    "hf_pids = hf.loc[:, ['pid']]\n",
    "#print(hf_pids)\n",
    "\n",
    "#print(~events['pid'].isin(hf_pids['pid']))\n",
    "\n",
    "ev_by_norm_patient = events.loc[~events['pid'].isin(hf_pids['pid']), :]\n",
    "#print(ev_by_norm_patient)\n",
    "ivid_norm_patient = ev_by_norm_patient.groupby(['pid'])[['vid']].max()\n",
    "#### Note: This seems ok\n",
    "#print(ivid_norm_patient)\n",
    "ivid_norm_patient.reset_index(drop=False, inplace=True)\n",
    "print(ivid_norm_patient)\n",
    "#print(ivid_norm_patient[ivid_norm_patient.index == 1230])\n",
    "#print(ivid_norm_patient[ivid_norm_patient.index == 78)\n",
    "#print(ivid_norm_patient.loc[ivid_norm_patient.loc[:, 'vid'] > 1, : ])\n",
    "\n",
    "#ev_by_hf_patient = events.loc[events['pid'].isin(hf_pids['pid']), ['pid', 'vid']]\n",
    "#print(ev_by_hf_patient.loc[ev_by_hf_patient.loc[:,'pid'] == 181, :])\n",
    "ev_by_hf_patient = pd.merge(events, hf,  how='inner',\n",
    "                      left_on=['pid','vid'], right_on = ['pid','vid'])\n",
    "\n",
    "print(ev_by_hf_patient.shape)\n",
    "#print(ev_by_hf_patient[ev_by_hf_patient.pid == 33])\n",
    "#ivid_hf_patient = ev_by_hf_patient.groupby(['pid'])[['vid']].nunique()\n",
    "ivid_hf_patient = ev_by_hf_patient.drop_duplicates(subset=['pid', 'vid'])\n",
    "ivid_hf_patient = ivid_hf_patient.loc[: , ['pid', 'vid']]\n",
    "ivid_hf_patient.reset_index(drop=True, inplace=True)\n",
    "print(ivid_hf_patient)\n",
    "\n",
    "indx_vid = pd.concat([ivid_norm_patient, ivid_hf_patient])\n",
    "#indx_vid['pid'] = indx_vid.index\n",
    "indx_vid.reset_index(drop=True, inplace=True)\n",
    "indx_vid.rename({'vid': 'indx_vid'}, axis=1, inplace=True)\n",
    "print(indx_vid)\n",
    "print(indx_vid[indx_vid.pid == 33])\n",
    "#print(x.loc[x.loc[:,'pid'] == 33, :].loc[x.loc[:,'vid'] == 0, :])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-17T04:30:01.464338Z",
     "start_time": "2022-01-17T04:30:01.162278Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "12811f08d77cb33b2b8821e3da47d7c9",
     "grade": true,
     "grade_id": "cell-7207b1c9d6f08a03",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "AUTOGRADER CELL. DO NOT MODIFY THIS.\n",
    "'''\n",
    "\n",
    "events, hf, feature_map = read_csv(TRAIN_DATA_PATH)\n",
    "indx_vid_df = calculate_index_vid(events, hf)\n",
    "assert indx_vid_df.shape == (4000, 2), \"calculate_index_vid failed!\"\n",
    "\n",
    "indx_vid = dict(list(zip(indx_vid_df.pid, indx_vid_df.indx_vid)))\n",
    "assert indx_vid[78] == 1, \"calculate_index_vid failed!\"\n",
    "assert indx_vid[1230] == 5, \"calculate_index_vid failed!\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c2bba556d9b1940fed3518e2f0b90e05",
     "grade": false,
     "grade_id": "cell-4a3b0c07c74dc8dc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 2.2 Filter events [10 points]\n",
    "\n",
    "Remove the events that occur outside the observation window. That is, all events in visits before index vid. Complete the method *filter_events* provided in the following code cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-17T04:30:01.469360Z",
     "start_time": "2022-01-17T04:30:01.465690Z"
    },
    "deletable": false
   },
   "outputs": [],
   "source": [
    "def filter_events(events, indx_vid):\n",
    "    \n",
    "    '''\n",
    "    TODO: This function needs to be completed.\n",
    "\n",
    "    Suggested steps:\n",
    "        1. Join indx_vid with events on pid.\n",
    "        2. Filter events occuring in the observation window [:, index vid) (Note that the right side is OPEN).\n",
    "    \n",
    "    \n",
    "    IMPORTANT:\n",
    "        `filtered_events` should be a pd dataframe withe header  `['pid', 'event_id', 'value']`.\n",
    "    '''\n",
    "\n",
    "    filtered_events = None\n",
    "    \n",
    "    # your code here\n",
    "    joined = pd.merge(events, indx_vid, on=\"pid\")\n",
    "\n",
    "    fev = joined.loc[(joined['vid'] < joined['indx_vid']), ['pid', 'event_id', 'value']]\n",
    "    filtered_events = fev.reset_index(drop=True)\n",
    "    \n",
    "    return filtered_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(682645, 4)\n",
      "(4000, 2)\n",
      "          pid  event_id  vid  value  indx_vid\n",
      "0          33  DIAG_244    0      1         0\n",
      "1          33  DIAG_414    0      1         0\n",
      "2          33  DIAG_427    0      1         0\n",
      "3          33  DIAG_585    0      1         0\n",
      "4          33  DIAG_V45    0      1         0\n",
      "...       ...       ...  ...    ...       ...\n",
      "682640  99971   DRUG_49    1      1         1\n",
      "682641  99971   DRUG_71    1      1         1\n",
      "682642  99971  DRUG_423    1      1         1\n",
      "682643  99971  DRUG_136    1      1         1\n",
      "682644  99971  DRUG_202    1      1         1\n",
      "\n",
      "[682645 rows x 5 columns]\n",
      "          pid  event_id  indx_vid\n",
      "0          78  DIAG_278         1\n",
      "1          78  DIAG_285         1\n",
      "2          78  DIAG_300         1\n",
      "3          78  DIAG_311         1\n",
      "4          78  DIAG_338         1\n",
      "...       ...       ...       ...\n",
      "169099  99971    DRUG_1         1\n",
      "169100  99971   DRUG_50         1\n",
      "169101  99971  DRUG_111         1\n",
      "169102  99971   DRUG_29         1\n",
      "169103  99971  DRUG_210         1\n",
      "\n",
      "[169104 rows x 3 columns]\n",
      "(281763, 3)\n",
      "(0, 3)\n",
      "(0, 3)\n",
      "(0, 3)\n"
     ]
    }
   ],
   "source": [
    "print(events.shape)\n",
    "print(indx_vid_df.shape)\n",
    "\n",
    "joined = pd.merge(events, indx_vid_df, on=\"pid\")\n",
    "print(joined)\n",
    "x=joined\n",
    "\n",
    "#print(x[(x['vid'] < x['indx_vid']) & (x['pid'] == 78)])\n",
    "\n",
    "fev = x.loc[(x['vid'] < x['indx_vid']), ['pid', 'event_id', 'indx_vid']]\n",
    "hf0 = x.loc[(x['indx_vid'] == 0), ['pid', 'event_id', 'indx_vid']]\n",
    "fev = fev.reset_index(drop=True)\n",
    "print(fev)\n",
    "print(hf0.shape)\n",
    "print(fev[fev.pid == 33].shape)\n",
    "print(hf0[hf0.pid == 78].shape)\n",
    "print(fev[fev.pid.isin(hf0['pid'])].shape)\n",
    "\n",
    "#print(x.isnull().values.any())\n",
    "#print(events.loc[events.loc[:,'pid'] == 33, :])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-17T04:30:02.026928Z",
     "start_time": "2022-01-17T04:30:01.472907Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a72ba59ec4465e07159f63371dcaaeca",
     "grade": true,
     "grade_id": "cell-a298eb0fe4ed28f9",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(169104, 3)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "AUTOGRADER CELL. DO NOT MODIFY THIS.\n",
    "'''\n",
    "\n",
    "events, hf, feature_map = read_csv(TRAIN_DATA_PATH)\n",
    "indx_vid = calculate_index_vid(events, hf)\n",
    "filtered_events = filter_events(events, indx_vid)\n",
    "print(filtered_events.shape)\n",
    "assert filtered_events[filtered_events.pid == 78].shape == (128, 3), \"filter_events failed!\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a74bda5c299d66e1901820f18bb9aa25",
     "grade": false,
     "grade_id": "cell-845f881f710c6b3b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 2.3 Aggregate events [10 points]\n",
    "\n",
    "To create features suitable for machine learning, we will need to aggregate the events for each patient as follows:\n",
    "\n",
    "- **count** occurences for each event.\n",
    "\n",
    "Each event type will become a feature and we will directly use event_id as feature name. For example, given below raw event sequence for a patient,\n",
    "\n",
    "```\n",
    "33,DIAG_244,0,1\n",
    "33,LAB_50971,0,1\n",
    "33,LAB_50931,0,1\n",
    "33,LAB_50931,0,1\n",
    "33,DIAG_244,1,1\n",
    "33,DIAG_427,1,1\n",
    "33,DRUG_0,1,1\n",
    "33,DRUG_3,1,1\n",
    "33,DRUG_3,1,1\n",
    "```\n",
    "\n",
    "We can get feature value pairs *(event_id, value)* for this patient with ID *33* as\n",
    "```\n",
    "(DIAG_244, 2.0)\n",
    "(LAB_50971, 1.0)\n",
    "(LAB_50931, 2.0)\n",
    "(DIAG_427, 1.0)\n",
    "(DRUG_0, 1.0)\n",
    "(DRUG_3, 2.0)\n",
    "```\n",
    "\n",
    "Next, replace each *event_id* with the *feature_id* provided in *event_feature_map.csv*.\n",
    "\n",
    "```\n",
    "(146, 2.0)\n",
    "(1434, 1.0)\n",
    "(1429, 2.0)\n",
    "(304, 1.0)\n",
    "(898, 1.0)\n",
    "(1119, 2.0)\n",
    "```\n",
    "\n",
    "Lastly, in machine learning algorithm like logistic regression, it is important to normalize different features into the same scale. We will use the [min-max normalization](http://stats.stackexchange.com/questions/70801/how-to-normalize-data-to-0-1-range) approach. (Note: we define $min(x)$ is always 0, i.e. the scale equation become $x$/$max(x)$).\n",
    "\n",
    "Complete the method *aggregate_events* provided in the following code cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-17T04:30:02.033863Z",
     "start_time": "2022-01-17T04:30:02.028268Z"
    },
    "deletable": false
   },
   "outputs": [],
   "source": [
    "def aggregate_events(filtered_events_df, hf_df, feature_map_df):\n",
    "    \n",
    "    '''\n",
    "    TODO: This function needs to be completed.\n",
    "\n",
    "    Suggested steps:\n",
    "        1. Replace event_id's with index available in event_feature_map.csv.\n",
    "        2. Aggregate events using count to calculate feature value.\n",
    "        3. Normalize the values obtained above using min-max normalization(the min value will be 0 in all scenarios).\n",
    "    \n",
    "    \n",
    "    IMPORTANT:\n",
    "        `aggregated_events` should be a pd dataframe with header `['pid', 'feature_id', 'feature_value']`.\n",
    "    '''\n",
    "    \n",
    "    aggregated_events = None\n",
    "    \n",
    "    # your code here\n",
    "    agg_events = filtered_events_df.groupby(['pid', 'event_id']).sum()\n",
    "    agg_events.reset_index(drop=False, inplace=True)\n",
    "\n",
    "    max_events = agg_events.loc[:, ['event_id', 'value']]\n",
    "    max_events = max_events.groupby(['event_id']).max()\n",
    "    max_events.reset_index(drop=False, inplace=True)\n",
    "    max_events = pd.merge(max_events, feature_map_df, on=\"event_id\")\n",
    "    max_events.rename({'value': 'max', 'idx': 'feature_id'}, axis=1, inplace=True)\n",
    "\n",
    "    joined_efm = pd.merge(agg_events, max_events, on=\"event_id\")\n",
    "    joined_efm['feature_value'] = joined_efm['value'] / joined_efm['max']\n",
    "\n",
    "    aggregated_events = joined_efm.loc[:, ['pid', 'feature_id', 'feature_value']]\n",
    "\n",
    "    return aggregated_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     pid  feature_id  feature_value\n",
      "0     78          20            1.0\n",
      "1     78         164            1.0\n",
      "2     78         175            1.0\n",
      "3    181         175            1.0\n",
      "4    190         175            1.0\n",
      "..   ...         ...            ...\n",
      "275  190        1419            1.0\n",
      "276  190        1425            1.0\n",
      "277  190        1434            1.0\n",
      "278  190        1445            1.0\n",
      "279  190        1446            1.0\n",
      "\n",
      "[280 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "events_in, hf_in, feature_map_in = read_csv(TRAIN_DATA_PATH)\n",
    "events_in = events_in.loc[:1000]\n",
    "hf_in = hf_in.loc[:100]\n",
    "\n",
    "index_vid = calculate_index_vid(events_in, hf_in)\n",
    "filtered_events = filter_events(events_in, index_vid)\n",
    "aggregated_events = aggregate_events(filtered_events, hf, feature_map_in)\n",
    "print(aggregated_events)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       pid   event_id  value\n",
      "1513  1230  DIAG_E879      1\n",
      "1595  1230  DIAG_E879      1\n",
      "1705  1230  DIAG_E879      1\n",
      "1786  1230  DIAG_E879      1\n",
      "14350\n",
      "                 value\n",
      "pid   event_id        \n",
      "72589 DIAG_070       7\n",
      "      DRUG_0         7\n",
      "      DRUG_186       7\n",
      "      DRUG_5         7\n",
      "      LAB_51301      7\n",
      "84188 DIAG_038       7\n",
      "      DIAG_041       7\n",
      "      DIAG_456       7\n",
      "      DIAG_584       7\n",
      "      DIAG_E879      7\n",
      "      DRUG_0         7\n",
      "      DRUG_1         7\n",
      "      DRUG_13        7\n",
      "      DRUG_6         7\n",
      "      DRUG_7         7\n",
      "      DRUG_73        7\n",
      "      LAB_51486      7\n",
      "      LAB_51491      7\n",
      "      LAB_51519      7\n",
      "          pid   event_id  value\n",
      "3384     2823  LAB_51519      3\n",
      "18482   11147  LAB_51519      2\n",
      "27559   16662  LAB_51519      2\n",
      "30180   18455  LAB_51519      2\n",
      "31143   19173  LAB_51519      2\n",
      "35830   21950  LAB_51519      2\n",
      "35954   21963  LAB_51519      2\n",
      "44168   27743  LAB_51519      2\n",
      "46971   29360  LAB_51519      2\n",
      "59796   37511  LAB_51519      3\n",
      "65966   41901  LAB_51519      2\n",
      "78954   49996  LAB_51519      2\n",
      "80194   50975  LAB_51519      2\n",
      "80655   51184  LAB_51519      3\n",
      "84593   53099  LAB_51519      2\n",
      "87374   54981  LAB_51519      2\n",
      "89813   56369  LAB_51519      2\n",
      "96923   60801  LAB_51519      2\n",
      "110695  70045  LAB_51519      2\n",
      "115458  73779  LAB_51519      2\n",
      "120905  77758  LAB_51519      2\n",
      "127597  81593  LAB_51519      2\n",
      "132000  84188  LAB_51519      7\n",
      "135117  87091  LAB_51519      2\n",
      "137578  88645  LAB_51519      2\n",
      "140587  90401  LAB_51519      2\n",
      "          pid   event_id  value\n",
      "0          78   DIAG_041      1\n",
      "1          78   DIAG_266      1\n",
      "2          78   DIAG_278      1\n",
      "3          78   DIAG_285      1\n",
      "4          78   DIAG_293      1\n",
      "...       ...        ...    ...\n",
      "154749  99971  LAB_51274      1\n",
      "154750  99971  LAB_51275      1\n",
      "154751  99971  LAB_51277      1\n",
      "154752  99971  LAB_51279      1\n",
      "154753  99971  LAB_51301      1\n",
      "\n",
      "[154754 rows x 3 columns]\n",
      "          pid   event_id  value\n",
      "132000  84188  LAB_51519      7\n",
      "       event_id  max  feature_id\n",
      "0      DIAG_004    1           1\n",
      "1      DIAG_007    1           3\n",
      "2      DIAG_008    2           4\n",
      "3      DIAG_009    1           5\n",
      "4      DIAG_018    1          10\n",
      "...         ...  ...         ...\n",
      "1247  LAB_51506    3        1468\n",
      "1248  LAB_51508    5        1469\n",
      "1249  LAB_51514    4        1470\n",
      "1250  LAB_51516    8        1471\n",
      "1251  LAB_51519    7        1472\n",
      "\n",
      "[1252 rows x 3 columns]\n",
      "          pid  event_id  value  max  feature_id  feature_value\n",
      "0          78  DIAG_041      1    7          20       0.142857\n",
      "1         415  DIAG_041      1    7          20       0.142857\n",
      "2        1178  DIAG_041      1    7          20       0.142857\n",
      "3        1230  DIAG_041      2    7          20       0.285714\n",
      "4        1343  DIAG_041      1    7          20       0.142857\n",
      "...       ...       ...    ...  ...         ...            ...\n",
      "154749  96546  DIAG_603      1    1         451       1.000000\n",
      "154750  98312  DIAG_445      1    1         321       1.000000\n",
      "154751  98847  DIAG_148      1    1          69       1.000000\n",
      "154752  98950  DIAG_977      2    2         729       1.000000\n",
      "154753  99492  DIAG_007      1    1           3       1.000000\n",
      "\n",
      "[154754 rows x 6 columns]\n",
      "          pid   event_id  value  max  feature_id  feature_value\n",
      "112428  84188  LAB_51519      7    7        1472            1.0\n",
      "          pid   event_id  value  max  feature_id  feature_value\n",
      "112461  90401  LAB_51519      2    7        1472       0.285714\n",
      "          pid  feature_id  feature_value\n",
      "0          78          20       0.142857\n",
      "1         415          20       0.142857\n",
      "2        1178          20       0.142857\n",
      "3        1230          20       0.285714\n",
      "4        1343          20       0.142857\n",
      "...       ...         ...            ...\n",
      "154749  96546         451       1.000000\n",
      "154750  98312         321       1.000000\n",
      "154751  98847          69       1.000000\n",
      "154752  98950         729       1.000000\n",
      "154753  99492           3       1.000000\n",
      "\n",
      "[154754 rows x 3 columns]\n",
      "          pid  feature_id  feature_value\n",
      "1042    88037         182       0.125000\n",
      "39070   88037        1400       0.500000\n",
      "40460   88037        1420       0.200000\n",
      "41794   88037        1424       0.166667\n",
      "47970   88037        1443       0.250000\n",
      "49953   88037        1447       0.125000\n",
      "51193   88037        1454       0.166667\n",
      "54880   88037        1468       0.333333\n",
      "58122   88037         410       0.500000\n",
      "71309   88037        1418       0.166667\n",
      "72990   88037        1430       0.200000\n",
      "74259   88037        1432       0.250000\n",
      "78141   88037        1442       0.200000\n",
      "80409   88037        1452       0.200000\n",
      "86215   88037        1425       0.200000\n",
      "87808   88037        1434       0.125000\n",
      "90174   88037        1446       0.200000\n",
      "91178   88037         371       0.333333\n",
      "96491   88037         173       0.166667\n",
      "105607  88037        1449       0.333333\n",
      "105892  88037         184       0.250000\n",
      "115869  88037        1409       0.250000\n",
      "121268  88037        1431       0.166667\n",
      "122318  88037        1450       0.166667\n",
      "131936  88037        1423       0.200000\n",
      "141939  88037         181       0.666667\n",
      "153493  88037         320       0.500000\n",
      "154662  88037          10       1.000000\n",
      "154738  88037           1       1.000000\n",
      "Empty DataFrame\n",
      "Columns: [pid, feature_id, feature_value]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "events, hf, feature_map = read_csv(TRAIN_DATA_PATH)\n",
    "index_vid = calculate_index_vid(events, hf)\n",
    "filtered_events = filter_events(events, index_vid)\n",
    "\n",
    "#print(feature_map.head())\n",
    "#print(events[events.pid == 33])\n",
    "#print(filtered_events[filtered_events.pid == 33])\n",
    "print(filtered_events[(filtered_events.pid == 1230) & (filtered_events.event_id == 'DIAG_E879')])\n",
    "#print(filtered_events.drop_duplicates(subset=['pid', 'event_id']))\n",
    "# 14350 are the number of events with more than one occurrence per patient\n",
    "print(filtered_events.shape[0] - 154754)\n",
    "\n",
    "#joined_efm = pd.merge(filtered_events, feature_map, on=\"event_id\")\n",
    "#print(joined_efm)\n",
    "#joined_efm['event_id'] = joined_efm['idx']\n",
    "agg_events = filtered_events.groupby(['pid', 'event_id']).sum()\n",
    "print(agg_events[agg_events.value == 7])\n",
    "\n",
    "#print(joined_efm[joined_efm.idx > 1])\n",
    "agg_events.reset_index(drop=False, inplace=True)\n",
    "print(agg_events[(agg_events.event_id == 'LAB_51519') & (agg_events.value > 1)])\n",
    "print(agg_events)\n",
    "\n",
    "\n",
    "print(agg_events[(agg_events.pid == 84188) & (agg_events.event_id == 'LAB_51519')])\n",
    "max_events = agg_events.loc[:, ['event_id', 'value']]\n",
    "max_events = max_events.groupby(['event_id']).max()\n",
    "max_events.reset_index(drop=False, inplace=True)\n",
    "max_events = pd.merge(max_events, feature_map, on=\"event_id\")\n",
    "max_events.rename({'value': 'max', 'idx': 'feature_id'}, axis=1, inplace=True)\n",
    "print(max_events)\n",
    "\n",
    "joined_efm = pd.merge(agg_events, max_events, on=\"event_id\")\n",
    "joined_efm['feature_value'] = joined_efm['value'] / joined_efm['max']\n",
    "print(joined_efm)\n",
    "print(joined_efm[(joined_efm.pid == 84188) & (joined_efm.event_id == 'LAB_51519')])\n",
    "print(joined_efm[(joined_efm.pid == 90401) & (joined_efm.event_id == 'LAB_51519')])\n",
    "\n",
    "aggregated_events = joined_efm.loc[:, ['pid', 'feature_id', 'feature_value']]\n",
    "print(aggregated_events)\n",
    "\n",
    "print(aggregated_events[aggregated_events.pid == 88037])\n",
    "print(aggregated_events[(aggregated_events.pid == 156)])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-17T04:30:02.786817Z",
     "start_time": "2022-01-17T04:30:02.035456Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f36119f0574d09e596cc891bd1f71bd4",
     "grade": true,
     "grade_id": "cell-b5931f772d6dac9f",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       pid  feature_id  feature_value\n",
      "0       78          20       0.142857\n",
      "255     78         164       1.000000\n",
      "261     78         175       0.250000\n",
      "432     78         182       0.125000\n",
      "1140    78         190       0.500000\n",
      "...    ...         ...            ...\n",
      "53620   78        1465       0.250000\n",
      "54085   78        1467       0.166667\n",
      "54454   78        1468       0.333333\n",
      "54939   78        1469       0.200000\n",
      "55416   78        1470       0.250000\n",
      "\n",
      "[127 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "AUTOGRADER CELL. DO NOT MODIFY THIS.\n",
    "'''\n",
    "\n",
    "events, hf, feature_map = read_csv(TRAIN_DATA_PATH)\n",
    "index_vid = calculate_index_vid(events, hf)\n",
    "filtered_events = filter_events(events, index_vid)\n",
    "aggregated_events = aggregate_events(filtered_events, hf, feature_map)\n",
    "print(aggregated_events[aggregated_events.pid == 78])\n",
    "assert aggregated_events[aggregated_events.pid == 88037].shape == (29, 3), \"aggregate_events failed!\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "57aa824b8fd9cf75ae7da47f0ff3156b",
     "grade": false,
     "grade_id": "cell-8db3e08c181b90cc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 2.4 Save in  SVMLight format [10 points]\n",
    "\n",
    "If the dimensionality of a feature vector is large but the feature vector is sparse (i.e. it has only a few nonzero elements), sparse representation should be employed. In this problem you will use the provided data for each patient to construct a feature vector and represent the feature vector in SVMLight format.\n",
    "\n",
    "```\n",
    "<line> .=. <target> <feature>:<value> <feature>:<value>\n",
    "<target> .=. 1 | 0\n",
    "<feature> .=. <integer>\n",
    "<value> .=. <float>\n",
    "```\n",
    "\n",
    "The target value and each of the feature/value pairs are separated by a space character. Feature/value pairs MUST be ordered by increasing feature number. **(Please do this in `save_svmlight()`.)** Features with value zero can be skipped. For example, the feature vector in SVMLight format will look like: \n",
    "\n",
    "```\n",
    "1 2:0.5 3:0.12 10:0.9 2000:0.3\n",
    "0 4:1.0 78:0.6 1009:0.2\n",
    "1 33:0.1 34:0.98 1000:0.8 3300:0.2\n",
    "1 34:0.1 389:0.32\n",
    "```\n",
    "\n",
    "where, 1 or 0 will indicate whether the patient has heart failure or not (i.e. the label) and it will be followed by a series of feature-value pairs **sorted** by the feature index (idx) value.\n",
    "\n",
    "You may find *utils.py* useful. You can review the code by running `%load utils.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-17T04:30:02.790053Z",
     "start_time": "2022-01-17T04:30:02.788376Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5615560ca2a7bb2e7ce7134dedb800ab",
     "grade": false,
     "grade_id": "cell-286e10c42da874be",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "%load   ./utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-17T04:30:03.114846Z",
     "start_time": "2022-01-17T04:30:02.791439Z"
    },
    "deletable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "import utils\n",
    "import collections\n",
    "\n",
    "def create_features(events_in, hf_in, feature_map_in):\n",
    "\n",
    "    indx_vid = calculate_index_vid(events_in, hf_in)\n",
    "\n",
    "    #Filter events in the observation window\n",
    "    filtered_events = filter_events(events_in, indx_vid)\n",
    "\n",
    "    #Aggregate the event values for each patient \n",
    "    aggregated_events = aggregate_events(filtered_events, hf_in, feature_map_in)\n",
    "\n",
    "    '''\n",
    "    TODO: Complete the code below by creating two dictionaries.\n",
    "        1. patient_features : Key is pid and value is array of tuples(feature_id, feature_value). \n",
    "                              Note that pid should be integer.\n",
    "        2. hf : Key is pid and value is heart failure label.\n",
    "    '''\n",
    "    patient_features = None\n",
    "    hf = None\n",
    "    \n",
    "    # your code here\n",
    "    aggregated_events['tuples'] = list(zip(aggregated_events.feature_id,\n",
    "                                           aggregated_events.feature_value))\n",
    "\n",
    "    tuppled = aggregated_events.groupby('pid')[['tuples']].aggregate(lambda f: list(f))\n",
    "    patient_features = dict(zip(tuppled.index, tuppled.tuples))\n",
    "\n",
    "    #unique_pids = aggregated_events.drop_duplicates(subset=['pid'])\n",
    "    #merged = pd.merge(unique_pids, hf_in, how='outer', on='pid')\n",
    "    #print(merged)\n",
    "    #hf_list = list(merged.pid.isin(hf_in.pid) * 1)\n",
    "    #print(sum(hf_list))\n",
    "\n",
    "    hf = dict(zip(hf_in.pid, [1] * hf_in.shape[0]))\n",
    "\n",
    "    return patient_features, hf\n",
    "\n",
    "def save_svmlight(patient_features, hf, op_file):\n",
    "    \n",
    "    '''\n",
    "    TODO: This function needs to be completed.\n",
    "\n",
    "    Create op_file: - which saves the features in svmlight format. (See instructions in section 2.4 for detailed explanatiom)\n",
    "    \n",
    "    Note: Please make sure the features are ordered in ascending order, and patients are stored in ascending order as well.     \n",
    "    To save the files, you could write:\n",
    "        deliverable.write(bytes(f\"{label} {feature_value} \\n\", 'utf-8'))\n",
    "    '''\n",
    "\n",
    "    deliverable = open(op_file, 'wb')\n",
    "    # your code here\n",
    "    for pid, features in patient_features.items():\n",
    "        target = hf.get(pid, 0)\n",
    "        sorted_features = sorted(features, key=lambda v: v[0])\n",
    "        feature_values = utils.bag_to_svmlight(sorted_features)\n",
    "        deliverable.write(bytes(f\"{target} {feature_values} \\n\", 'utf-8'))\n",
    "\n",
    "    deliverable.close()\n",
    "\n",
    "\n",
    "events_in, hf_in, feature_map_in = read_csv(TRAIN_DATA_PATH)\n",
    "patient_features, hf = create_features(events_in, hf_in, feature_map_in)\n",
    "save_svmlight(patient_features, hf, 'features_svmlight.train')\n",
    "\n",
    "events_in, hf_in, feature_map_in = read_csv(VAL_DATA_PATH)\n",
    "patient_features, hf = create_features(events_in, hf_in, feature_map_in)\n",
    "save_svmlight(patient_features, hf, 'features_svmlight.val')\n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-17T04:30:03.318468Z",
     "start_time": "2022-01-17T04:30:03.115935Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f36390a5de4e9e4ed4395f8cef946bd7",
     "grade": true,
     "grade_id": "cell-5f208e4ea6e5a154",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "AUTOGRADER CELL. DO NOT MODIFY THIS.\n",
    "'''\n",
    "\n",
    "events_in, hf_in, feature_map_in = read_csv(TRAIN_DATA_PATH)\n",
    "events_in = events_in.loc[:1000]\n",
    "hf_in = hf_in.loc[:100]\n",
    "patient_features, hf = create_features(events_in, hf_in, feature_map_in)\n",
    "print(len(hf))\n",
    "assert 78 in patient_features, \"create_features is missing patients\"\n",
    "assert len(patient_features[78]) == 127, \"create_features is wrong\"\n",
    "assert patient_features[78][:5] == [(20, 1.0), (164, 1.0), (175, 1.0), (182, 1.0), (190, 1.0)], \"create_features is wrong\"\n",
    "assert len(hf) == 101, \"create_features is wrong\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cce7c0a2c651e4e8cddad28f5f0dbc0e",
     "grade": false,
     "grade_id": "cell-480980a849ebd089",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The whole pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-17T04:30:18.478851Z",
     "start_time": "2022-01-17T04:30:03.320257Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d511eacce421e41b3f8e0a5d0e69639a",
     "grade": false,
     "grade_id": "cell-5be75ccfce6fe003",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    events_in, hf_in, feature_map_in = read_csv(TRAIN_DATA_PATH)\n",
    "    patient_features, hf = create_features(events_in, hf_in, feature_map_in)\n",
    "    save_svmlight(patient_features, hf, 'features_svmlight.train')\n",
    "    \n",
    "    events_in, hf_in, feature_map_in = read_csv(VAL_DATA_PATH)\n",
    "    patient_features, hf = create_features(events_in, hf_in, feature_map_in)\n",
    "    save_svmlight(patient_features, hf, 'features_svmlight.val')\n",
    "    \n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c28533d723309779b8530cb354815f28",
     "grade": false,
     "grade_id": "cell-a02bc53e8da63b0b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 3 Predictive Modeling [40 points]\n",
    "\n",
    "Make sure you have finished section 2 before you start to work on this question because some of the files generated in section 2 (*features_svmlight.train*) will be used in this question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "94422b239845574543cc7842983e0b86",
     "grade": false,
     "grade_id": "cell-87c0e61ad837e42c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 3.1 Model Creation [20 points]\n",
    "\n",
    "In the previous question, you constructed feature vectors for patients to be used as training data in various predictive models (classifiers). Now you will use this training data (*features_svmlight.train*) in 3 predictive models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9cc834d3630587624b8a4bcfe680de97",
     "grade": false,
     "grade_id": "cell-4c04be8133e4b75f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Step - a. Implement Logistic Regression, SVM and Decision Tree. Skeleton code is provided in the following code cell.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-17T04:30:18.840336Z",
     "start_time": "2022-01-17T04:30:18.480730Z"
    },
    "deletable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2485, 1473)\n",
      "[0. 1. 0. ... 0. 1. 1.]\n",
      "[0. 1. 0. ... 0. 1. 1.]\n",
      "(2485,)\n",
      "0.703420523138833\n",
      "0.703420523138833\n",
      "0.6657355679702048\n",
      "______________________________________________\n",
      "Classifier: Logistic Regression\n",
      "Accuracy: 0.856338028169014\n",
      "Precision: 0.8357933579335793\n",
      "Recall: 0.937888198757764\n",
      "F1-score: 0.8839024390243903\n",
      "______________________________________________\n",
      "\n",
      "______________________________________________\n",
      "Classifier: SVM\n",
      "Accuracy: 0.9070422535211268\n",
      "Precision: 0.896484375\n",
      "Recall: 0.9503105590062112\n",
      "F1-score: 0.9226130653266331\n",
      "______________________________________________\n",
      "\n",
      "______________________________________________\n",
      "Classifier: Decision Tree\n",
      "Accuracy: 0.703420523138833\n",
      "Precision: 0.6657355679702048\n",
      "Recall: 0.9868875086266391\n",
      "F1-score: 0.7951070336391437\n",
      "______________________________________________\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import *\n",
    "\n",
    "import utils\n",
    "\n",
    "\n",
    "# PLEASE USE THE GIVEN FUNCTION NAME, DO NOT CHANGE IT.\n",
    "# USE THIS RANDOM STATE FOR ALL OF THE PREDICTIVE MODELS.\n",
    "# OR THE TESTS WILL NEVER PASS.\n",
    "RANDOM_STATE = 545510477\n",
    "\n",
    "\n",
    "#input: X_train, Y_train\n",
    "#output: Y_pred\n",
    "def logistic_regression_pred(X_train, Y_train):\n",
    "    \n",
    "    \"\"\"\n",
    "    TODO: Train a logistic regression classifier using X_train and Y_train.\n",
    "    Use this to predict labels of X_train. Use default params for the classifier.\n",
    "    \"\"\"\n",
    "    \n",
    "    # your code here\n",
    "    clf = LogisticRegression(random_state=RANDOM_STATE).fit(X_train, Y_train)\n",
    "    return clf.predict(X_train)\n",
    "\n",
    "X_train, Y_train = utils.get_data_from_svmlight(\"features_svmlight.train\")\n",
    "print(X_train.shape)\n",
    "print(logistic_regression_pred(X_train, Y_train))\n",
    "#display_metrics(\"Logistic Regression\", logistic_regression_pred(X_train, Y_train), Y_train)\n",
    "\n",
    "#input: X_train, Y_train\n",
    "#output: Y_pred\n",
    "def svm_pred(X_train, Y_train):\n",
    "    \n",
    "    \"\"\"\n",
    "    TODO: Train a SVM classifier using X_train and Y_train.\n",
    "    Use this to predict labels of X_train. Use default params for the classifier\n",
    "    \"\"\"\n",
    "    \n",
    "    # your code here\n",
    "    clf = LinearSVC(random_state=RANDOM_STATE).fit(X_train, Y_train)\n",
    "    return clf.predict(X_train)\n",
    "\n",
    "print(svm_pred(X_train, Y_train))\n",
    "#input: X_train, Y_train\n",
    "#output: Y_pred\n",
    "def decisionTree_pred(X_train, Y_train):\n",
    "\n",
    "    \"\"\"\n",
    "    TODO: Train a logistic regression classifier using X_train and Y_train.\n",
    "    Use this to predict labels of X_train. Use max_depth as 5.\n",
    "    \"\"\"\n",
    "    \n",
    "    # your code here\n",
    "    clf = DecisionTreeClassifier(random_state=RANDOM_STATE, max_depth=5)\\\n",
    "        .fit(X_train, Y_train)\n",
    "    return clf.predict(X_train)\n",
    "\n",
    "preds = decisionTree_pred(X_train, Y_train)\n",
    "\n",
    "accuracy = Y_train == preds\n",
    "print(accuracy.shape)\n",
    "print(accuracy.mean())\n",
    "print(accuracy_score(Y_train, preds))\n",
    "print(precision_score(Y_train, preds))\n",
    "#input: Y_pred,Y_true\n",
    "#output: accuracy, precision, recall, f1-score\n",
    "def classification_metrics(Y_pred, Y_true):\n",
    "    \n",
    "    \"\"\"\n",
    "    TODO: Calculate the above mentioned metrics.\n",
    "    NOTE: It is important to provide the output in the same order.\n",
    "    \"\"\"\n",
    "    \n",
    "    # your code here\n",
    "    accuracy = accuracy_score(Y_true, Y_pred)\n",
    "    precision = precision_score(Y_true, Y_pred)\n",
    "    recall = recall_score(Y_true, Y_pred)\n",
    "    f1 = f1_score(Y_true, Y_pred)\n",
    "\n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "    \n",
    "#input: Name of classifier, predicted labels, actual labels\n",
    "def display_metrics(classifierName, Y_pred, Y_true):\n",
    "    print(\"______________________________________________\")\n",
    "    print((\"Classifier: \"+classifierName))\n",
    "    acc, precision, recall, f1score = classification_metrics(Y_pred,Y_true)\n",
    "    print((\"Accuracy: \"+str(acc)))\n",
    "    print((\"Precision: \"+str(precision)))\n",
    "    print((\"Recall: \"+str(recall)))\n",
    "    print((\"F1-score: \"+str(f1score)))\n",
    "    print(\"______________________________________________\")\n",
    "    print(\"\")\n",
    "\n",
    "    \n",
    "def main():\n",
    "    X_train, Y_train = utils.get_data_from_svmlight(\"features_svmlight.train\")\n",
    "\n",
    "    display_metrics(\"Logistic Regression\", logistic_regression_pred(X_train, Y_train), Y_train)\n",
    "    display_metrics(\"SVM\",svm_pred(X_train, Y_train),Y_train)\n",
    "    display_metrics(\"Decision Tree\", decisionTree_pred(X_train, Y_train), Y_train)\n",
    "\n",
    "    \n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-17T04:30:19.121904Z",
     "start_time": "2022-01-17T04:30:18.842373Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "034aed31d22576811319cd685d665149",
     "grade": true,
     "grade_id": "cell-c59bbb34681f832b",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "AUTOGRADER CELL. DO NOT MODIFY THIS.\n",
    "'''\n",
    "\n",
    "from utils import get_data_from_svmlight\n",
    "from numpy.testing import assert_almost_equal\n",
    "\n",
    "### 3.1a Training Accuracy [3 points]\n",
    "X_train, Y_train = get_data_from_svmlight(\"features_svmlight.train\")\n",
    "\n",
    "# test_accuracy_lr\n",
    "expected = 0.856338028169014\n",
    "Y_pred = logistic_regression_pred(X_train, Y_train)\n",
    "actual = classification_metrics(Y_pred, Y_train)[0]\n",
    "assert_almost_equal(actual, expected, decimal=2, verbose=False, err_msg=\"test_accuracy_lr failed!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "69dd38d29d3e7c0035782d6569b16c3b",
     "grade": false,
     "grade_id": "cell-f6628dbab9bc57d4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Step - b. Evaluate your predictive models on a separate test dataset in *features_svmlight.val* (binary labels are provided in that svmlight file as the first field). Skeleton code is provided in the following code cell.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-17T04:30:19.404126Z",
     "start_time": "2022-01-17T04:30:19.123750Z"
    },
    "deletable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "______________________________________________\n",
      "Classifier: Logistic Regression\n",
      "Accuracy: 0.6937086092715232\n",
      "Precision: 0.7345360824742269\n",
      "Recall: 0.776566757493188\n",
      "F1-score: 0.7549668874172186\n",
      "______________________________________________\n",
      "\n",
      "______________________________________________\n",
      "Classifier: SVM\n",
      "Accuracy: 0.640728476821192\n",
      "Precision: 0.7038043478260869\n",
      "Recall: 0.7057220708446866\n",
      "F1-score: 0.7047619047619047\n",
      "______________________________________________\n",
      "\n",
      "______________________________________________\n",
      "Classifier: Decision Tree\n",
      "Accuracy: 0.6821192052980133\n",
      "Precision: 0.6611418047882136\n",
      "Recall: 0.9782016348773842\n",
      "F1-score: 0.789010989010989\n",
      "______________________________________________\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import *\n",
    "\n",
    "import utils\n",
    "\n",
    "\n",
    "# PLEASE USE THE GIVEN FUNCTION NAME, DO NOT CHANGE IT.\n",
    "# USE THIS RANDOM STATE FOR ALL OF THE PREDICTIVE MODELS.\n",
    "# OR THE TESTS WILL NEVER PASS.\n",
    "RANDOM_STATE = 545510477\n",
    "\n",
    "\n",
    "#input: X_train, Y_train and X_test\n",
    "#output: Y_pred\n",
    "def logistic_regression_pred(X_train, Y_train, X_test):\n",
    "\n",
    "    \"\"\"\n",
    "    TODO: train a logistic regression classifier using X_train and Y_train. \n",
    "    Use this to predict labels of X_test use default params for the classifier.\n",
    "    \"\"\"\n",
    "    \n",
    "    # your code here\n",
    "    clf = LogisticRegression(random_state=RANDOM_STATE).fit(X_train, Y_train)\n",
    "    return clf.predict(X_test)\n",
    "    \n",
    "\n",
    "#input: X_train, Y_train and X_test\n",
    "#output: Y_pred\n",
    "def svm_pred(X_train, Y_train, X_test):\n",
    "    \n",
    "    \"\"\"\n",
    "    TODO: Train a SVM classifier using X_train and Y_train.\n",
    "    Use this to predict labels of X_test use default params for the classifier.\n",
    "    \"\"\"\n",
    "    \n",
    "    # your code here\n",
    "    clf = LinearSVC(random_state=RANDOM_STATE).fit(X_train, Y_train)\n",
    "    return clf.predict(X_test)\n",
    "\n",
    "    \n",
    "#input: X_train, Y_train and X_test\n",
    "#output: Y_pred\n",
    "def decisionTree_pred(X_train, Y_train, X_test):\n",
    "    \n",
    "    \"\"\"\n",
    "    TODO: Train a logistic regression classifier using X_train and Y_train.\n",
    "    Use this to predict labels of X_test.\n",
    "    IMPORTANT: use max_depth as 5. Else your test cases might fail.\n",
    "    \"\"\"\n",
    "    \n",
    "    # your code here\n",
    "    clf = DecisionTreeClassifier(random_state=RANDOM_STATE, max_depth=5)\\\n",
    "        .fit(X_train, Y_train)\n",
    "    return clf.predict(X_test)\n",
    "\n",
    "\n",
    "#input: Y_pred,Y_true\n",
    "#output: accuracy, precision, recall, f1-score\n",
    "def classification_metrics(Y_pred, Y_true):\n",
    "    \n",
    "    \"\"\"\n",
    "    TODO: Calculate the above mentioned metrics.\n",
    "    NOTE: It is important to provide the output in the same order.\n",
    "    \"\"\"\n",
    "    \n",
    "    # your code here\n",
    "    accuracy = accuracy_score(Y_true, Y_pred)\n",
    "    precision = precision_score(Y_true, Y_pred)\n",
    "    recall = recall_score(Y_true, Y_pred)\n",
    "    f1 = f1_score(Y_true, Y_pred)\n",
    "\n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "    \n",
    "#input: Name of classifier, predicted labels, actual labels\n",
    "def display_metrics(classifierName, Y_pred, Y_true):\n",
    "    print(\"______________________________________________\")\n",
    "    print((\"Classifier: \"+classifierName))\n",
    "    acc, precision, recall, f1score = classification_metrics(Y_pred,Y_true)\n",
    "    print((\"Accuracy: \"+str(acc)))\n",
    "    print((\"Precision: \"+str(precision)))\n",
    "    print((\"Recall: \"+str(recall)))\n",
    "    print((\"F1-score: \"+str(f1score)))\n",
    "    print(\"______________________________________________\")\n",
    "    print(\"\")\n",
    "\n",
    "    \n",
    "def main():\n",
    "    X_train, Y_train = utils.get_data_from_svmlight(\"features_svmlight.train\")\n",
    "    X_test, Y_test = utils.get_data_from_svmlight(os.path.join(\"features_svmlight.val\"))\n",
    "\n",
    "    display_metrics(\"Logistic Regression\", logistic_regression_pred(X_train, Y_train, X_test), Y_test)\n",
    "    display_metrics(\"SVM\", svm_pred(X_train, Y_train, X_test), Y_test)\n",
    "    display_metrics(\"Decision Tree\", decisionTree_pred(X_train, Y_train, X_test), Y_test)\n",
    "\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-17T04:30:19.677389Z",
     "start_time": "2022-01-17T04:30:19.406083Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b37be2e239bb84e1dbe3df96764e934e",
     "grade": true,
     "grade_id": "cell-f23ffe89a0cd304a",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "AUTOGRADER CELL. DO NOT MODIFY THIS.\n",
    "'''\n",
    "\n",
    "from utils import get_data_from_svmlight\n",
    "from numpy.testing import assert_almost_equal\n",
    "\n",
    "### 3.1b Prediction Accuracy [3 points]\n",
    "X_train, Y_train = get_data_from_svmlight(\"features_svmlight.train\")\n",
    "X_test, Y_test = get_data_from_svmlight(\"features_svmlight.val\")\n",
    "\n",
    "# test_accuracy_lr\n",
    "expected = 0.6937086092715232\n",
    "Y_pred = logistic_regression_pred(X_train, Y_train, X_test)\n",
    "actual = classification_metrics(Y_pred, Y_test)[0]\n",
    "assert_almost_equal(actual, expected, decimal=2, verbose=False, err_msg=\"test_accuracy_lr failed!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f417499e53d12d286cd07c58cb33130e",
     "grade": false,
     "grade_id": "cell-1a6ab34b3605550e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 3.2 Model Validation [20 points]\n",
    "\n",
    "In order to fully utilize the available data and obtain more reliable results, machine learning practitioners use cross-validation to evaluate and improve their predictive models. You will demonstrate using two cross-validation strategies against SVD. \n",
    "\n",
    "- K-fold: Divide all the data into $k$ groups of samples. Each time $\\frac{1}{k}$ samples will be used as test data and the remaining samples as training data.\n",
    "- Randomized K-fold: Iteratively random shuffle the whole dataset and use top specific percentage of data as training and the rest as test. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6a87293971a18439d3a0aabf105fa235",
     "grade": false,
     "grade_id": "cell-795340da3ac71dd4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Implement the two cross-validation strategies.**\n",
    "- **K-fold:** Use the number of iterations k=5; \n",
    "- **Randomized K-fold**: Use a test data percentage of 20\\% and k=5 for the number of iterations for Randomized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-17T04:30:20.379732Z",
     "start_time": "2022-01-17T04:30:19.679182Z"
    },
    "deletable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2485, 1473)\n",
      "0.7258461959533061\n",
      "Classifier: SVD\n",
      "Average F1 Score in KFold CV: 0.7258461959533061\n",
      "1988 497\n",
      "1988 497\n",
      "1988 497\n",
      "1988 497\n",
      "1988 497\n",
      "Average F1 Score in Randomised CV: 0.7195678940019832\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold, ShuffleSplit\n",
    "from numpy import mean\n",
    "\n",
    "import utils\n",
    "\n",
    "\n",
    "# PLEASE USE THE GIVEN FUNCTION NAME, DO NOT CHANGE IT.\n",
    "# USE THIS RANDOM STATE FOR ALL OF THE PREDICTIVE MODELS.\n",
    "# OR THE TESTS WILL NEVER PASS.\n",
    "RANDOM_STATE = 545510477\n",
    "\n",
    "\n",
    "#input: training data and corresponding labels\n",
    "#output: accuracy, f1\n",
    "def get_f1_kfold(X, Y, k=5):\n",
    "    \n",
    "    \"\"\"\n",
    "    TODO: First get the train indices and test indices for each iteration.\n",
    "    Then train the classifier accordingly.\n",
    "    Report the mean f1 score of all the folds.\n",
    "    \n",
    "    Note that you do not need to set random_state for KFold, as it has no effect since shuffle is False by default.\n",
    "    \"\"\"\n",
    "    \n",
    "    # your code here\n",
    "    kf = KFold(n_splits=k)\n",
    "    f1t = 0\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        Y_train, Y_test = Y[train_index], Y[test_index]\n",
    "        clf = LinearSVC(random_state=RANDOM_STATE).fit(X_train, Y_train)\n",
    "        Y_pred = clf.predict(X_test)\n",
    "        f1t = f1t + f1_score(Y_test, Y_pred)\n",
    "\n",
    "    return f1t / k\n",
    "\n",
    "X,Y = utils.get_data_from_svmlight(\"features_svmlight.train\")\n",
    "print(X.shape)\n",
    "print(get_f1_kfold(X, Y))\n",
    "    \n",
    "#input: training data and corresponding labels\n",
    "#output: accuracy, f1\n",
    "def get_f1_randomisedCV(X, Y, iterNo=5, test_percent=0.20):\n",
    "\n",
    "    \"\"\"\n",
    "    TODO: First get the train indices and test indices for each iteration.\n",
    "    Then train the classifier accordingly.\n",
    "    Report the mean f1 score of all the iterations.\n",
    "    \n",
    "    Note that you need to set random_state for ShuffleSplit\n",
    "    \"\"\"\n",
    "\n",
    "    # your code here\n",
    "    ss = ShuffleSplit(n_splits=iterNo, random_state=RANDOM_STATE, test_size=test_percent)\n",
    "    f1t = 0\n",
    "    for train_index, test_index in ss.split(X):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        Y_train, Y_test = Y[train_index], Y[test_index]\n",
    "        clf = LinearSVC(random_state=RANDOM_STATE).fit(X_train, Y_train)\n",
    "        Y_pred = clf.predict(X_test)\n",
    "        f1t = f1t + f1_score(Y_test, Y_pred)\n",
    "\n",
    "    return f1t / iterNo\n",
    "\n",
    "    \n",
    "def main():\n",
    "    X,Y = utils.get_data_from_svmlight(\"features_svmlight.train\")\n",
    "    print(\"Classifier: SVD\")\n",
    "    f1_k = get_f1_kfold(X,Y)\n",
    "    print((\"Average F1 Score in KFold CV: \"+str(f1_k)))\n",
    "    f1_r = get_f1_randomisedCV(X,Y)\n",
    "    print((\"Average F1 Score in Randomised CV: \"+str(f1_r)))\n",
    "\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-17T04:30:21.105767Z",
     "start_time": "2022-01-17T04:30:20.381024Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "35f7fbef1e0f7512541358d3a261b838",
     "grade": true,
     "grade_id": "cell-c7bfbc42bf80f768",
     "locked": true,
     "points": 20,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "AUTOGRADER CELL. DO NOT MODIFY THIS.\n",
    "'''\n",
    "\n",
    "from numpy.testing import assert_almost_equal\n",
    "\n",
    "### 3.2 Cross Validation F1 [10 points]\n",
    "# test_f1_cv_kfold\n",
    "expected = 0.7258461959533061\n",
    "X, Y = get_data_from_svmlight(\"features_svmlight.train\")\n",
    "actual = get_f1_kfold(X, Y)\n",
    "assert_almost_equal(actual, expected, decimal=2, verbose=False, err_msg=\"test_f1_cv_kfold failed!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-17T04:30:21.109723Z",
     "start_time": "2022-01-17T04:30:21.106920Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cb382fd95d36e5a8b937b7769bb7c5c0",
     "grade": true,
     "grade_id": "cell-8ebda70b8d984a2f",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "illinois_payload": {
   "b64z": "",
   "nb_path": "release/HW1/HW1.ipynb"
  },
  "kernelspec": {
   "display_name": "Python 3 (Threads: 2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "832px",
    "left": "419px",
    "top": "110px",
    "width": "311.390625px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}